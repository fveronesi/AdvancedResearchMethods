---
title: "Advanced Research Methods - E7004"
author: "Dr. Fabio Veronesi"
date: "19 March 2018"
output:
  word_document: default
  pdf_document: default
  html_document: default
subtitle: Day 1 - Introduction
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=F)
```

##**Summary**
- Introduction
- Data Structures
- Data types
- Subsetting
- Importing Data
- Descriptive Statistics
- Plotting

***
####*NOTE*
This document has been written using R Markdown. More info on R Markdown syntax at:

[basic-writing-and-formatting-syntax](https://help.github.com/articles/basic-writing-and-formatting-syntax)

[Markdown-Here-Cheatsheet](https://github.com/adam-p/markdown-here/wiki/Markdown-Here-Cheatsheet#lists)

[rmarkdown-cheatsheet](https://www.rstudio.com/wp-content/uploads/2015/02/rmarkdown-cheatsheet.pdf)

[rmarkdown-reference](https://www.rstudio.com/wp-content/uploads/2015/03/rmarkdown-reference.pdf)

***

##**Introduction**
R is a powerful object-oriented programming language, specifically designed for statistics. Object-oriented means that we can assign data to named objects and use them for computations.

It is free and open-source and can be downloaded for free at:
[CRAN](https://cran.r-project.org/)

I also suggest to download R Studio, which makes writing code much easier:
[R-Studio](https://www.rstudio.com/)

***

##**Data Structures**
More info on data structures here:
[venus.ifca.unican.es](http://venus.ifca.unican.es/Rintro/dataStruct.html)

The most important data structures are vector, matrix, data.frame and lists (array is not used much)
![Source: venus.ifca.unican.es](https://github.com/fveronesi/AdvancedResearchMethods/raw/Images/dataStructuresNew.png)


###**Vectors**
Vectors are simply 1D arrays of any type, e.g. numerical, categorical or strings.
```{r}
A = rnorm(n=20, mean=4, sd=1)

A
```

A is a vector with 40 elements. We created this vector by using the function `rnorm`, which randomly samples `n = 20` numerical values from a normal distribution (`mean = 4, sd = 1`)


```{r}
length(A)
```

We can obtain the length, i.e. the number of elements, in the vector using the function `length`. `A` is an object, specifically a 1D array also referred to as `vector` in R, and it can be used as such for analysis:


```{r}
B = A + 3

B
```

This expression adds the number 3 to each element of the `vector`, and stores this new `vector` into an object named `B`.

###**Matrix**
Data can also be stored into more complex objects, like `matrix` and `data.frame`:


```{r}
M = matrix(1:40, nrow=8, ncol=5)
```

This line creates a 8x5 `matrix`, which is a 2D object with a total of 40 elements.

We can look at it using the function `print`:


```{r}
print(M)
```

Please notice the way R fills the matrix. It does so by column. We can change that using the option `byrow`:

```{r}
M = matrix(1:40, nrow=8, ncol=5, byrow=T)
print(M)
```

The function `help` can be very useful when we are not familiar with options available:

```{r, eval=F}
help(matrix)
```
  


###**Data.Frame**
Other important objects are `data.frame`:

```{r}
head(iris)
```

`iris` is one of the numerous datasets freely available in R that we can use for testing. The function `head` allows us to just look at the first few rows of the dataset.

As you can see this object has multiple columns and many rows, but the entries are not only `numeric`.

```{r}
str(iris)
```


The function `str` allows us to visualize the structure of any object, and it is particularly useful for `data.frame`. It tells us that this dataset has 150 observations (rows) and 5 variables (columns). It also tell us that the first 4 variables are numerical (`num`), while the last is categorical (or factorial).

Other types of variables could be logical (e.g. `TRUE` or `FALSE`), or strings (e.g. "TEST")


###**List**
The last data structure we are going to look at is `list`. This structure allows to store diverse data types:


```{r}
L = list(A, M)
print(L)
```

This list has two elements, the first is a `vector` while the second is a `matrix`.

***
##**Subsetting**
We can use square brackets to extract elements from all data structures. However, the syntax changes between objects.

`Vector` are 1D arrays so we can extract elements just by specifying the position along the `vector` we want to extract, starting from 1 up to `length(vector)`:


```{r}
A[1]
```

This extracts the first element of the `vector`.


```{r}
A[length(A)]
```

This extracts the last element, in position equal to `length(A)`, which is 40.

For `matrix` and `data.frame`, since these are 2D objects, we need to specify both a row and a column to extract:

```{r}
M[3,5]

iris[4,4]
```

We can also extract multiple elements:

```{r}
iris[1:4, 2]
```


This line extracts from row 1 to row 4, but only elements in column 2.

We can also extract entire rows or columns. For example: 

```{r}
M[1,]
```

extracts the whole row 1. While:

```{r}
M[,3]
```

extracts the whole column 3.

In a `data.frame` we can also extract columns by name:

```{r}
iris$Sepal.Length
```

For lists, since these are complex objects, we have two levels of extraction:

```{r}
L[[1]]
```


The first levels, identified by 2 square brackets, extracts the element of the list (in this case element 1, which is a vector).
Then we can extract a particular element from the vector adding an additional level:

```{r}
L[[1]][1]
```


The same would be true for the second element, which is a `data.frame`:

```{r}
L[[2]][3,5]
```

In this case, since the second element has both rows and columns, the second level of extraction needs to include two numbers.


***
##**Importing Data**
In this course we will use data freely available on-line, which can be downloaded from: [sheffield.ac.uk](https://www.sheffield.ac.uk/mash/statistics2/data). A short description for each dataset is also available.

In case we have data stored locally, we first need to specify our working directory so that R knows where to load and save data. We can do that via the function `setwd`:

```{r, eval=F}
setwd("J:/Teaching_Harper/Advanced Research Methods - E7004/Possible Datasets")
```

Please notice the forward slash (`/`) instead of the back slash we would normally use. Another way is go to Session->Set Working Directory directly in R Studio. Now we can import our data. The easiest way to import tables is from csv files.

This can also be done directly from the website:

```{r}
Crimes = read.csv("https://www.sheffield.ac.uk/polopoly_fs/1.569434!/file/stcp-Rdataset-Crime.csv")

str(Crimes)
```


Please notice the name of the first variable `ï..CrimeRate`. This sometimes happens in R, which probably has some issues with this particular file.
It is important to notice this, because we always need to use accurate variable names in R, otherwise the script will not work. We can find the description of the dataset at this link: [Crime_data_summary](https://www.sheffield.ac.uk/polopoly_fs/1.547014!/file/Crime_data_summary.docx)

Even though reading csv is the easiest way to import data, there are many other formats of data that can be imported into R (e.g. Excel sheet). To find out more on how to import different formats of data please look at the links below:

[Quick-R](https://www.statmethods.net/input/importingdata.html)

[DataCamp](https://www.datacamp.com/community/tutorials/r-data-import-tutorial)


***
##**Descriptive Statistics**
Descriptive statistics is a basic technique that is used to describe a dataset.  
This technique relies on indexes to capture important features of the dataset, such as mean and standard deviation, which will help us to compare it with other data.

###**Centrality**
The most popular index of centrality is the mean, or arithmetic average:

###$\displaystyle\bar{y} = \frac{1}{n} \times \sum_{i=1}^{n} y_i$

where $\bar{y}$ is the mean of a variable $y$, and $n$ is the number of elements in $y$.

***
####_Note_
The previous equation was written using the Latex syntax.
More info here: [Latex Equation Reference](https://en.wikibooks.org/wiki/LaTeX/Mathematics#Powers_and_indices)

***

The function `mean` can be used in R to compute the arithmetic average:
```{r}
mean(Crimes$ï..CrimeRate)
```

Another useful measure of centrality is the median, which is defined as the middle value of a series of numbers, listed in numerical order.
We can simply compute it using the function `median` in R:

```{r}
median(Crimes$ï..CrimeRate)
```


###**Spread**
The Variance is a measure of spread and indicates the average amount of dispersion, i.e. distance from the mean, of each value in a dataset.
It can be simply computed as follows:

####$\displaystyle s^2 = \frac{1}{n-1} \times \sum_{i=1}^{n} {(y_i - \bar{y})}^2$

By definition, the variance is sum of the square differences between each observation $y_i$ and the mean $\bar{y}$, divided by the number of observations minus 1.

In R, the variance can be simply computed as follows:
```{r}
var(Crimes$ï..CrimeRate)
```

The Standard Deviation ($s$) is the square root of the variance:
```{r}
sd(Crimes$ï..CrimeRate)
```


###**Summary**
Sometimes, instead of computing indexes for a single variable we are more interested in looking at the whole dataset at once. We can do that with the function `summary`:
```{r}
summary(Crimes)
```

This function returns several important information for all the variables in the `data.frame` `Crimes`. Beside mean and median, it also reports the minimum and maximum values, and the first and third quartiles, which are useful to understand the shape of the distribution.

###**Quantiles**
Quantiles are just cut points that divide the probability distribution into equal probability intervals:

![Source: mathspace.co](https://github.com/fveronesi/AdvancedResearchMethods/raw/Images/78th_percentile2.png)

In this example, the $78^{th}$ percentile is the point below which $78%$ of observations lie. Important for statistics is the division of the distribution into four parts, in this case we talk about quartiles. These are important because they allow to measure spread for non-normal distribution: using the distance between first and third quartiles (interquartile range):

![Source: wikipedia](https://github.com/fveronesi/AdvancedResearchMethods/raw/Images/598px-Boxplot_vs_PDF.svg.png)

The interquartile range can be easily computed in R with the function `IQR`:

```{r}
IQR(Crimes$ï..CrimeRate)
```


***
##**Plotting**
The final part of this lecture is dedicated to plotting. Plots are extremely important because they allow us to graphically represent our data, which is extremely important.

There are many useful plots that we will use throughout the course. In particular we will look at histograms and box-plots, which allows us to visualise and compare distributions; bar-charts, which are useful to develop hypothesis particularly for categorical data; and scatterplots, which are very useful to understand the relation between variables.

###**Histograms**
Histograms are plots designed to represent distributions. To create a histogram we simply need to divide our data into non-overlapping "bins", meaning small intervals, which will be plotted on the X axis. Then we count the number of values that fall into each bin, and this number is plotted on the Y axis.

Let's look at an example, again using the `ï..CrimeRate` variable:
```{r}
hist(Crimes$ï..CrimeRate)
```

As you can see the histogram has split the data into 7 bins, and plot the frequency (i.e. the number of values within each bin) on the Y axis.

We can add elements to this plots by changing title and the label for the X axis, with the options `main` and `xlab`:
```{r}
hist(Crimes$ï..CrimeRate, main="Histogram Crime Rates", xlab="Crime rate (number of offences per million population)")
```

As mentioned, histograms are very useful to check the shape of the distribution. For example, in this case we can see that the distribution is very close to normal, with its typical bell shape. This is extremely useful particularly when we will start looking at statistical tests. In fact, many tests are specifically designed to deal only with normally distributed data.

###**Bar Charts**
Bar charts are very useful when we need to compare groups. In the dataset Crimes there are variables that are binary, i.e. either 0 or 1. For example, `Southern` divides  crimes reported in Southern (1) and Northern (0) states. Let's say we want to compare average crimes in both areas. For this task bar charts are the right tools.

Before we can plot the data we need to compute averages for both categories in the variable `Southern`. We can do it with the function `tapply`: 

```{r}
Mean = tapply(X=Crimes$ï..CrimeRate, INDEX=Crimes$Southern, FUN=mean)
Mean
```

The function `tapply`, applies a function (`FUN`) to a continuous variable (`X`), divided by groups (`INDEX`). Once we have the mean we can plot bars using `barplot`:

```{r}
barplot(Mean, names.arg=c("Northern States", "Southern States"))
```

It seems Northern states have slightly more crimes. 

We can provide more details on the plot by creating error bars, for example using the standard error of the mean.
The standard error is computed as follows:

###$\displaystyle SEM = \frac{s}{\sqrt{n}}$

where $s$ is the standard deviation, and $n$ is the number of samples.

In base R there is no function to compute the standard error, but we can create one:

```{r}
SEM = function(x){sd(x) / sqrt(length(x))}
```


This simple function will accept a vector `x`, as only input. Then it will compute the standard error as a ratio between the standard deviation (`sd`), and the square root (`sqrt`) of the number of elements (`length`) of the vector `x`.
Now that we have a function we can again use `tapply`, to compute standard errors for each category:
```{r}
SEM.STATES = tapply(X=Crimes$ï..CrimeRate, INDEX=Crimes$Southern, FUN=SEM)
SEM.STATES
```

These can be added to the plot with `segments` and `arrows`. However, we first need to recall the plot and save it into an object:

```{r}
BC_PLOT = barplot(Mean, names.arg=c("Northern States", "Southern States"), ylim=c(0,120))
```

The object `BC_PLOT` now contains the plot, and in particular the coordinates of the two bars. We also added the option `ylim`, to manually set the range of the Y axis. This way we can make sure the error bars are fully visible. 
```{r}
BC_PLOT
```


With these numbers we can specify where to insert the error bars, as twice the standard error of the mean:
```{r}
BC_PLOT = barplot(Mean, names.arg=c("Northern States", "Southern States"), ylim=c(0,120))

segments(x0=BC_PLOT, y0=Mean - (2*SEM.STATES), x1=BC_PLOT, y1=Mean + (2*SEM.STATES), lwd=1.5)

arrows(x0=BC_PLOT, y0=Mean - (2*SEM.STATES), x1=BC_PLOT, y1=Mean + (2*SEM.STATES), lwd=1.5, angle=90, code=3, length=0.05)
```


To know more about the code in the last two functions please refer to:
[www.r-bloggers.com/building-barplots-with-error-bars/](https://www.r-bloggers.com/building-barplots-with-error-bars/)

Plotting confidence intervals around the mean is extremely useful since it may provide good indications of statistical differences between groups. In fact, SEM is used in inferential statistics to compare groups. If two error bars are overlapping there are good chances that with a formal test the two groups will not result statistically different. This is something we can use to formulate our hypothesis before applying any statistical test.


###**Scatterplot**
Scatterplots allows us to visualise the relation between two continuous variables. For example, let's say we want to visualise the relation between Crime rate and Wages, we can do that simply by:

```{r}
plot(ï..CrimeRate ~ Wage10, data=Crimes, pch=20, xlab="Median Weekly Wage", ylab="Crime rate")
```


It seems there is a positive correlation between the two, i.e. when one increases the other also increases. This sort of plots is extremely useful in hypothesis testing, since it allows us to immediately figure out that some variables may be driving the variance we see in our target variable, in this case Crime rates.


###**Box-Plots**
Box-Plots are another way to represent distributions, this time using quartiles. They are also very useful to compare distributions.

First of all, let's look at the R code to generate box-plots:
```{r}
boxplot(Crimes$ï..CrimeRate, main="Histogram Crime Rates", xlab="Crime rate (number of offences per million population)")
```


Box-plots are a clever way of representing distributions. They feature a box, which ranges from the first (`Q1`) to the third quartile (`Q3`), thus representing the interquartile range (measure of spread valid also for non-normal distributions). Inside there is a black line that corresponds to the median. Then we have the two whiskers, which extend up to a distance of 1.5 times the interquartile range below `Q1` and above `Q3`. Outside this range we sometimes see dots that are considered outliers.

The image below illustrates how to interpret box-plots:

![Source: lsc.studysixsigma.com](https://github.com/fveronesi/AdvancedResearchMethods/raw/Images/1435.png)

We can increase the amount of information displayed by adding a red dot on the mean value. This can be done with the function `points`:
```{r}
boxplot(Crimes$ï..CrimeRate, main="Histogram Crime Rates", xlab="Crime rate (number of offences per million population)")
points(mean(Crimes$ï..CrimeRate), pch=16, col="red")
```


As you can see the function `points` is written after the call to create the plot. Moreover, within the call to `points` we have two more options: `pch` and `col`. The first, `pch`, is used to change the symbol used for the dot and it can be any of the symbols below:

![pch symbols, source: isu.r-forge.r-project.org](https://github.com/fveronesi/AdvancedResearchMethods/raw/Images/pch.png)

The option `col` is used to change the colour of the symbol, which by default is black. We can either write the colour: e.g. red, green, yellow. Alternatively, we can select more advanced colours from the range below:

![Author: Earl F. Glynn (2005)](https://github.com/fveronesi/AdvancedResearchMethods/raw/Images/ColorsChart3.jpg)

Box plots are a very good way of comparing distributions of different groups. We can do that including a formula, for example specifying that we want to plot distributions of Crime Rate as a function of Youth unemployment:

```{r}
boxplot(Crimes$ï..CrimeRate  ~ Crimes$HighYouthUnemploy, main="Histogram Crime Rates", xlab="Crime rate (number of offences per million population)")
```

Box plots can be used with any distribution. However, their role become very important when we are dealing with non-normal data. In fact, with normal distributions we can simply use a bar-chart to plot the mean values of each group, and an error bar with the confidence interval (computed as twice the standard error of the mean). However, for data that do not fit a normal distribution this is not possible and this is where box plot become very useful. In fact, using the interquartile range (`IQR`) we can compute confidence intervals around the median, with the following equation:

####$CI_{Median} = Median \pm 1.57 \times \frac{IQR}{\sqrt{n}}$

Confidence intervals around the median can be plotted in R as notches, with the option `notch`:

```{r}
boxplot(Crimes$ï..CrimeRate  ~ Crimes$HighYouthUnemploy, notch=T, main="Histogram Crime Rates", xlab="Crime rate (number of offences per million population)")
```

The interpretation is very similar to the confidence intervals around the mean. If the two notches are overlapping there are good chances that with a formal test the two groups will not result statistically different.


***
##**Conclusions**
In this lecture we learned the basics of the R language, such as importing and subsetting data. We also started learning useful technique to explore our dataset, both in terms of descriptive indexes, and by visualising our variables.



***
##**References**
Material to implement what we discussed during lectures can be downloaded from my OneDrive.

I also developed a video tutorial for Pack Publishing, which again students can download from OneDrive:

[OneDrive](https://goo.gl/Yfr1Y3)

***
##**Homework**
Homework are meant to help you during the learning process. They will not be marked and they are not mandatory.
However, if you have time and you want to do some additional work to better understand the R language these are some things you can try. Please save all your homework, from each day, into a single file: name_surname_StudentID.R

Please identify each day with specific comments. A comment in R can be included with the hash-tag (#):
```
#Comment: you can write whatever neccesary for me to understand your reasoning
```

At the end of the module you can send the R file to me: fveronesi@harper-adams.ac.uk

You can find the list of homework below:

1. From this [page](https://www.sheffield.ac.uk/mash/statistics2/data), download the Diet dataset and place it in a folder of your choice. Then set the working directory pointing to the same folder where you downloaded the dataset. Finally load the dataset using `read.csv`.

2. Subset the dataset by gender (be careful for the NA values) and compute the mean values for the variable `weight6weeks` for both subsets.

3. Create a bar-chart using the two subsets.

4. Create a scatterplot between `pre.weight` and `weight6weeks`.

5. Comment the scatterplot and the relation between variables.